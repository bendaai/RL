强化学习的训练信号是用来评估给定动作的好坏的，而不是通过正确动作示范来进行直接的指导，这使得主动反复试验以试探出好的动作变得很有必要。
## 1. 问题描述
重复在k个选项或动作中进行选择，每次做出选择后，你都会得到一个收益，收益由你选择的动作决定的平稳概率分布产生【action->P(val)】,你的目标是在某段时间内最大化总收益的期望【老虎机】
### 1.1 基本符号定义
$A_t$:t时刻选择的动作
$R_t:A_t$对应的收益
$q_* (a)$:任一动作a对应的价值,即给定动作a时，收益的期望
$$q_* (a)=E[R_t | A_t=a]$$
$Q_t(a)$:对$q_* (a)$的估计
### 1.2 如何选择动作
#### 1.2.1 贪心
每次选$Q_t(a)$最高的
### 1.2.2 试探
选择非贪心的动作
## 2.动作-价值方法
> 使用价值的估计来进行动作的选择【估计价值->选择动作】

### 2.1 价值估计
一种自然的方式是通过实际收益的平均值来估计动作的价值,即动作a的总收益/a的次数
$$
Q_t(a)=\frac{\sum_{i=1}^{t-1} R_i* 1_{A_i=a}}{\sum_{i=1}^{t-1}  1_{A_i=a}}
$$
分母为0时，我们假定$Q_t(a)=0$，当分母趋向于无穷大时，根据大数定律，$Q_t(a)->q_* (a)$。
由于每一次估计都是对相关利益样本的平均，因此我们称其为采样平均方法，这是一种价值评估的方法，并不是最好的方法，我们暂时通过该评估方法来讨论如何使用评估值来选择动作。
### 2.2 动作选择
#### 2.2.1 贪心
$$
A_t=\mathop{\arg\max}_ {a} Q_t(a)
$$
利用当前知识最大化眼前的利益，由于不去试探，可能每次只是选择了次优的动作，因此$Q_t(a)->q_* (a)$可能不成立
#### 2.2.2 近乎贪心
$\epsilon$-贪心,以$\epsilon$的概率不使用贪心规则，而从所有动作中等概率做出选择，这样一来，如果时刻无限长，所有动作都会被无限次采样，从而确保$Q_t(a)->q_* (a)$，即选择最优动作的概率收敛于$1-\epsilon$
#### 2.2.3 如何选择规则
$\epsilon$的选择更依赖于任务，比如老虎机每个臂收益的方差为0，那么每次选择都是$q_* (a)$，贪心方法在尝试一次后就知道每个动作的真实价值，这种情况下，贪心方法可能表现最好。
>非平稳性

动作的真实价值会随着时间而变化，这种情况下，即使是确定性的情况下，试探也是需要的

#### 2.2.4 增量学习【优化计算】
以高效的方式计算样本均值进行估计
$$
Q_{n+1}=\frac{Q_n*(n-1)+R_n}{n}=Q_n+\frac{R_n-Q_n}{n}
$$
新估计值 <- 旧估计值 + 步长 x [ 目标 - 旧估计值 ]

### 2.3 非平稳问题
给近期的收益赋予一个比过去很久的收益更高的权值是一种合理的处理方式，最流行的一种方法是使用固定步长即$\frac{1}{n}->\alpha\in(0,1]$,这个方法也被称为指数近因加权平均
$$
Q_{n+1}=Q_n+\alpha (R_n-Q_n)=\alpha R_n+(1-\alpha)Q_n=...=(1-\alpha)^nQ_1+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}R_i
$$
有时候随时刻一步步改变步长参数，即设$\alpha_n(a)$为处理第n次选择动作a后收益的步长参数。当$\alpha_n(a)=\frac{1}{n}$将会得到采样平均法
可见，估计是受到初值$Q_1$ 影响的，而$Q_1$ 只能由我们的经验设定，因此实际上估计是有偏的(但是渐进无偏)。合理选择初值，有助于算法找到更好的解。对于采样均值(sample-average)方法，这种偏差会随着每个动作被选择而消失。对于常数步长方法来说，这种偏差则消失得慢很多。
#### 2.3.0  无偏恒定步长技巧【练习2.7】
用如下步长处理某个特定动作的第n个收益
$$
\beta_n=\frac{\alpha}{\bar{o_n}},\bar{o_n}=\bar{o_{n-1}}+\alpha(1-\bar{o_{n-1}})
$$
证明：
$$
\begin{aligned}
Q_{n+1} =& Q_n+\beta_n(R_n-Q_n)\\
&=\beta_nR_n+(1-\beta_n)Q_n\\
&=\beta_nR_n+(1-\beta_n)\beta_{n-1}R_{n-1}+(1-\beta_n)(1-\beta_{n-1})Q_{n-1}\\
&=...\\
&=\sum_{i=1}^n (\beta_iR_i \prod_{j=i+1}^{n}(1-\beta_j))+\prod_{i=1}^{n}(1-\beta_i)Q_1\\
\beta_1&=\frac{\alpha}{\bar{o_1}}=\frac{\alpha}{0+\alpha}=1\\
\Rightarrow\\
Q_{n+1} =& \sum_{i=1}^n (\beta_iR_i \prod_{j=i+1}^{n}(1-\beta_j))\\

\end{aligned}
$$
### 2.4 乐观初始化
初始化时对估计价值进行乐观的选择，这样会让学习器开始的时候对得到的收益感到失望，即下次会选择另一个动作进行修正，这样，所有动作在估计值收敛前都被尝试好几次，即使按照贪心法选择动作，也类似于进行了大量的试探。
这个方法不适用与非平稳问题，因为它试探的驱动力是暂时的，开始时刻只出现一次。

### 2.5 基于置信度上界的动作选择【UCB算法】
非贪心动作中，根据他们的潜力来选择可能事实上最优的动作，这需要考虑他们的估计有多接近最大值，以及这些估计的不确定性。
一个有效的方式按如下公式选择动作
$$
A_t=\mathop{\arg\max}_ {a} [Q_t(a)+c\sqrt{\frac{In\ t}{N_t(a)}}
$$
$N_t(a)$是t时刻前a被选择的次数，显然如果没被选择过，则认为是满足最大化条件的动作。c是大于0的一个数，用于控制试探的程度

#### 2.5.1 缺点
和$\epsilon-$贪心相比，更难推广到其他研究的更一般的问题中，一个困难是处理非平稳问题时，需要更复杂的方法，另一个困难是要处理大的状态空间，即更复杂的问题时，目前还没有已知使用的方法利用UCB动作选择的思想

### 2.6 梯度赌博机算法【另一种评估动作价值的算法】
>使用估计值来选择动作通常是个好方法，但不是唯一可使用的方法

#### 2.6.1 介绍
针对每个动作a考虑学习一个数值化的偏好函数$H_t(a)$，偏好函数越大，动作就越频繁的被选择，其概念并不是从收益意义上出发的，只有一个动作对另一个动作的相对偏好才是重要的，即如果我们队每个动作偏好函数都加上1000，则对于下列分布确定的动作概率是没有任何影响的
$$
\pi_t(a)=P(A_t=a)=\frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}
$$
其中，$\pi_t(a)$用来表示动作a在t时刻被选中的概率。
#### 2.6.2 算法描述
初始化:$H_1(a)=0,\forall a$,即每个动作被选择的概率是相同的
基于随机梯度上升的思想的一种更新方式：
每个步骤中，在选择动作$A_t$并获得利益$R_t$之后，偏好函数按如下方式更新
$$
\begin{aligned}
H_{t+1}(A_t) &=H_t(A_t)+\alpha(R_t-\bar{R_t})(1-\pi_t(A_t))\\
H_{t+1}(a)   &=H_t(a)-\alpha(R_t-\bar{R_t})\pi_t(a),a\neq A_t\\
\end{aligned}
$$
其中$\alpha$是一个大于0的数，表示步长,$\bar{R_t}$是时刻t内所有收益的平均值，其作为一个基准项，若收益高于它，那么未来选择$A_t$的概率会增加，反之概率会降低。
证明：
$$
\begin{aligned}
H_{t+1}(a) &=H_t(a)+\alpha \frac{\partial E[R_t]}{\partial H_t(a)}\\
\frac{\partial E[R_t]}{\partial H_t(a)} &=\frac{\partial}{\partial H_t(a)} [\sum_x\pi_t(x)q_* (x)]\\
&=\sum_x(q_* (x))\frac{\partial \pi_t(x)}{\partial H_t(a)}\\
&=\sum_x(q_* (x)-B_t)\frac{\partial \pi_t(x)}{\partial H_t(a)} ,because \sum_x\pi_t(x)=1\\
&=\sum_x \pi_t(x)(q_* (x)-B_t)\frac{\partial \pi_t(x)}{\partial H_t(a)}/\pi_t(x)\\
&=E_{A_t\sim \pi_t(x)}[(q_* (x)-B_t)\frac{\partial \pi_t(x)}{\partial H_t(a)}/\pi_t(x)]\\
&=E_{A_t\sim \pi_t(x)}[(R_t-\bar{R_t})\frac{\partial \pi_t(x)}{\partial H_t(a)}/\pi_t(x)]\\

\frac{\partial \pi_t(x)}{\partial H_t(a)}&=\frac{\partial}{\partial H_t(a)}[\frac{e^{}H_t(x)}{\sum_{y=1}^{k}e^{H_t(y)}}]\\
&=\frac{1_{a=x}e^{H_t(x)}\sum_{y=1}^{k}e^{H_t(y)}-e^{H_t(x)}e^{H_t(a)}}{(\sum_{y=1}^{k}e^{H_t(y)})^2}\\
&=1_{a=x}\pi_t(x)-\pi_t(x)\pi_t(a)\\
&=\pi_t(x)(1_{a=x}-\pi_t(a))\\
\frac{\partial E[R_t]}{\partial H_t(a)} &=E_{A_t\sim \pi_t(x)}[(R_t-\bar{R_t})\pi_t(x)(1_{a=x}-\pi_t(a))/\pi_t(x)]\\
&=E_{A_t\sim \pi_t(x)}[(R_t-\bar{R_t})(1_{a=x}-\pi_t(a))]\\
We\ assum\ &E_{A_t\sim \pi_t(x)}[(R_t-\bar{R_t})(1_{a=x}-\pi_t(a))]=(R_t-\bar{R_t})(1_{a=x}-\pi_t(a))\\
H_{t+1}(a) &=H_t(a)+\alpha(R_t-\bar{R_t})(1_{a=x}-\pi_t(a))
\end{aligned}
$$


### 2.7 关联搜索【上下文相关的赌博机】
>既采用试错学习去搜索最有动作，又将这些动作与他们表现最优时的情境关联起来。

一系列不同的k臂赌博机，每一步先随机选择其中的一个。因此需要学习一些任务相关的策略，即把每个任务和该任务下的最有动作关联起来。比如，如果选择的赌博机是绿色的，那么选择2号臂，如果是红色的，选1号臂。

关联搜索处于k臂赌博机问题和完整强化学习问题之间
与赌博机的相似点：每个动作只影响吉时利益
与完整强化学习相似点：需要学习一种策略

如果允许动作可以影响下一时刻的情境和收益，那么就是完整的强化学习问题了。